# Text Prediction


```{r textPred}
source('textPred.R')
```

## Data acquisition and cleaning


### Obtaining the data

First we download and clean data with the following script.

```{r downloadData}
if (!file.exists('data')) dir.create('data')
zip.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zip.fn <- "data/Coursera-SwiftKey.zip"
if (!file.exists(zip.fn)) download.file(zip.url, destfile=zip.fn, method="curl")
if (!file.exists('data/final')) unzip(zip.fn, exdir='data')
```

### Cleaning

Now it is time for cleaning. We will read the file that contains
tweets in English. Then we transform the vector into data.table, since
the operations we will do later results to be faster. We will also
cache the data.table file.

```{r readData}
read.datafile <- function(fn, n=-1L) {
  txts <- readLines(fn, n)
  data.table(words=txts)
}
```

Next, we lower case of all letter, and then we deal with is
punctuation. What we are going to treat symbols `. , ? ... ; ! : () "`
as the one that divide the message (another future strategy is to only
remove them). We also include here a lonely `-`. Then we remove extra
empty spaces and finally we tokenize them. Here we use function
`basicDT` implemented in separated file.


```{r tokenization}
getTokens <- function(vn, fn, n=-1L) {
  if (dbExists(dbCache, vn)) {
    v <- dbFetch(dbCache, vn)
  } else {
    v <- read.datafile(fn, n) %>>%
      basicDT()
    dbInsert(dbCache, vn, v)
  }
  v
}
```

```{r tus}
tus.tokens <-
  getTokens("tus.tokens", 
            "data/final/en_US/en_US.twitter.txt", n=-1L)
bus.tokens <-
  getTokens("bus.tokens", 
            "data/final/en_US/en_US.blogs.txt", n=-1L)
nus.tokens <-
  getTokens("nus.tokens", 
            "data/final/en_US/en_US.news.txt", n=-1L)
```

## Exploratory analysis

### Frequencies of tokens

```{r freq}
tokens.freq <- function(dt) {
  dt[,.(counts=.N),by=tokens1][order(-counts)]
}
```

```{r}
tus.freq <- tokens.freq(tus.tokens)
bus.freq <- tokens.freq(bus.tokens)
nus.freq <- tokens.freq(nus.tokens)
```

```{r}
plotFreq <- function(dt) {
  p <- ggplot(dt) +
    geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
    xlab("Words")+ylab('Frequencies') +
    coord_flip()
}
```

#### The most frequnts words in (from left) tweets, blogs, news

```{r}
pt <- plotFreq(tus.freq[1:40, .(words=tokens1, counts)])
pb <- plotFreq(bus.freq[1:40, .(words=tokens1, counts)])
pn <- plotFreq(nus.freq[1:40, .(words=tokens1, counts)])
grid.arrange(pt, pb, pn, ncol=3)
```

### Most popular words in tweets

```{r worldcloud}

wordcloud(tus.freq[1:500, tokens1], tus.freq[1:500, counts],
          scale=c(5,0.5),
          random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

### Ngrams

Next we compute 2,3 and 4 grams using function implemented in
separated file.


```{r ngrams}
tus.ngrams <- ngramDTs(tus.tokens, 4)

```

Now let's check which are the most popular 2-grams, 3-grams and
4-grams in tweets.

```{r 2grams}
tus.2 <- tus.ngrams[[2]][,.(counts=.N),by=.(tokens1, tokens2)][order(-counts),]
tus.2.head <- tus.2[1:40,.(words=paste(tokens1, tokens2), counts)]
p2 <- plotFreq(tus.2.head)
p2

```

```{r 3grams}
tus.3 <- tus.ngrams[[3]][,.(counts=.N),by=.(tokens1, tokens2, tokens3)][order(-counts),]
tus.3.head <- tus.3[1:40,.(words=paste(tokens1, tokens2, tokens3), counts)]
p3 <- plotFreq(tus.3.head)
p3

```

```{r 4grams}
tus.4 <- tus.ngrams[[4]][,.(counts=.N),by=.(tokens1, tokens2, tokens3, tokens4)][order(-counts),]
tus.4.head <- tus.4[1:40,.(words=paste(tokens1, tokens2, tokens3, tokens4), counts)]
p4 <- plotFreq(tus.4.head)
p4

```

### Plans

Once we have 2,3 and 4 grams calculated, we are planing to use Katz's
back-off model together with Good-Turing estimation. This allows us to
choose in a smooth way between predictions obtained from n-grams
models with different n. We plan to use n up to 5.

For faster prediction we use data.table package. We will also use
filehash packages for storing computed frequency tables.

