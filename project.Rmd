# Text Prediction

## Introduction

This is an exploratory analysis for first milestone of Data Science
Capstone Project. If you are interesting in the full code see the last
section of this document.

```{r textPred}
source('textPred.R')
```

## Three data sets

We have three data sets. All of them are in English (except few
words). The first on contains tweets, the second one blog's notes and
the last one news. The final goal is to use those data sets to write
an application that, given few words, can predict next one(s). However
this document contains a basic exploratory analysis of the data.

### Tweets Data

#### File has the following number of lines:

```{r}
fn <- 'data/final/en_US/en_US.twitter.txt'
system(paste("wc -l", fn), intern = TRUE)
```

#### The total number of words is:
```{r}
dt <- getTokensCsv("tus.tokens")
dim(dt)[1]
```

#### The number of unique words is:
```{r}
dtSummary <- tokens.freq(dt)
rm(dt)
dim(dtSummary)[1]
```

#### The most popular words:

```{r worldcloud}
wordcloud(dtSummary[1:500, tokens1], dtSummary[1:500, counts],
          scale=c(5,0.5),
          random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

```{r}
plotFreq(dtSummary[1:40, .(words=tokens1, freq)])
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[, cumulativeFreq:=cumsum(counts)/sum(counts)]%>%
  .[cumulativeFreq<0.5, tokens1] %>%
  length
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[cumulativeFreq<0.9, tokens1] %>%
  length
```

```{r}
rm(dtSummary)
```
### News Data

#### File has the following number of lines:

```{r}
fn <- 'data/final/en_US/en_US.news.txt'
system(paste("wc -l", fn), intern = TRUE)
```

#### The total number of words is:
```{r}
dt <- getTokensCsv("nus.tokens")
dim(dt)[1]
```

#### The number of unique words is:
```{r}
dtSummary <- tokens.freq(dt)
rm(dt)
dim(dtSummary)[1]
```

#### The most popular words:

```{r worldcloudNews}
wordcloud(dtSummary[1:500, tokens1], dtSummary[1:500, counts],
          scale=c(5,0.5),
          random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

```{r}
plotFreq(dtSummary[1:40, .(words=tokens1, freq)])
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[, cumulativeFreq:=cumsum(counts)/sum(counts)]%>%
  .[cumulativeFreq<0.5, tokens1] %>%
  length
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[cumulativeFreq<0.9, tokens1] %>%
  length
```

```{r}
rm(dtSummary)
```

### Blogs Data

#### File has the following number of lines:

```{r}
fn <- 'data/final/en_US/en_US.blogs.txt'
system(paste("wc -l", fn), intern = TRUE)
```

#### The total number of words is:
```{r}
dt <- getTokensCsv("bus.tokens")
dim(dt)[1]
```

#### The number of unique words is:
```{r}
dtSummary <- tokens.freq(dt)
rm(dt)
dim(dtSummary)[1]
```

#### The most popular words:

```{r worldcloudBlog}
wordcloud(dtSummary[1:500, tokens1], dtSummary[1:500, counts],
          scale=c(5,0.5),
          random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

```{r}
plotFreq(dtSummary[1:40, .(words=tokens1, freq)])
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[, cumulativeFreq:=cumsum(counts)/sum(counts)]%>%
  .[cumulativeFreq<0.5, tokens1] %>%
  length
```

The following number of words cover 50% of words instances in the data
set:
```{r}
dtSummary[cumulativeFreq<0.9, tokens1] %>%
  length
```

```{r}
rm(dtSummary)
```

# Plans

Once we have 2,3 and 4 grams calculated, we are planing to use Katz's
back-off model together with Good-Turing estimation. This allows us to
choose in a smooth way between predictions obtained from n-grams
models with different n. We plan to use n up to 5.

At the moment the code that create ngrams is slow and buggy. We would
like to improve by use external scripts written in bash or C.

# How we wrote this report

## Completed Code

The code is available at
https://github.com/sbartek/textPrediction In particular, the script
`procUS.R` is responsible for downloading and it is preprocess data
using functions that are included in the file `textPred.R`.

## Steps

First we download data using the function `downloadCourseraSwiftKey`
which we implemented in `textPred.R`.

Now it was time for cleaning. We have read the file, then we transform
the vector into data.table, since the operations we were faster (the
most remarkable is `fread`).

Next, we lower case of all letter, and then we deal with is
punctuation. What we did is that we treated symbols `. , ? ... ; ! : () "`
as the one that divide the message (another future strategy is to only
remove them). We also include here a lonely `-`. Then we remove extra
empty spaces and finally we tokenize them. Here we use function
`basicDT` also implemented in `textPred.R`.
