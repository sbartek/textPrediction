## Introduction

```{r libraries}
require("tm")
require("pipeR")
require('wordcloud')
require("data.table")
require('ggplot2')
require('filehash')
require('RWeka')
```

### Cache

```{r filecache}
cache.fn <- "cache/dbCache"
if (!file.exists('cache')) dir.create('cache')
if (!file.exists(cache.fn)) dbCreate(cache.fn)
dbCache <- dbInit(cache.fn)
```

## Data acquisition and cleaning


### Obtaining the data

First we download and clean data with the following script.

```{r downloadData}
if (!file.exists('data')) dir.create('data')
zip.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zip.fn <- "data/Coursera-SwiftKey.zip"
if (!file.exists(zip.fn)) download.file(zip.url, destfile=zip.fn, method="curl")
if (!file.exists('data/final')) unzip(zip.fn, exdir='data')
```

### Cleaning

Now it is time for cleaning. Let us see what we have. We will read
1000 tweets in English in order to see what we have, since reading
entire file takes about 3 minutes on my computer. Then we transform
the vector into data.table, since the operations we will do later
results to be faster.

```{r}
read.datafile <- function(fn, n=-1L) {
  txts <- readLines(fn, n)
  data.table(words=txts)
}

tus <- read.datafile('data/final/en_US/en_US.twitter.txt', 1000)
tus0 <- read.datafile('data/final/en_US/en_US.twitter.txt')

```

Next, we lower case of all letter, and then we deal with is punctuation. What we are
going to to is to treat symbols `. , ? ... ; ! : () "` as the one that divide
the message. We also include here a lonely `-`.

Finally we remove extra empty spaces.

```{r}

splitPunctuation <- function(txts) {
  unlist(strsplit(txts, "\\.+|\\?+|!+|;+|,+"))
}

trim <- function (txts) gsub("^\\s+|\\s+$", "", txts)

removeExtraSpace <- function(txts) {
  gsub("\\s\\s+", " ", txts)
}

txts.clean <- function(txts) {
  txts %>>%
    tolower %>>%
    splitPunctuation %>>%
    removeExtraSpace %>>%
    trim
}

system.time({
  tus2 <- tus0[,.(words=txts.clean(words))][words!=""]
})
```

```{r processingFunctions}




removePunct <- function(txts) {
  gsub("[[:punct:]]", "", txts)
}

process.txts <- function(txts) {
  txts %>>%
    removePunctuation %>>%
    tolower %>>%
    removeNumbers %>>%
    removeWords(stopwords('english')) %>>%
    stripWhitespace %>>%
    trim
}

txts.tokenization <- function(txts) {
  txts  %>>%
    MC_tokenizer %>>%
    (.[.!=""])
}

words.freq <- function(words) {
  words.dt <- data.table(words=words)
  words.dt[,.(counts=.N),by=words][order(-counts)]
}

```

## Frequencies of tokens

```{r load_files}

## aprox 11 s

system.time({
  txts <- readLines('data/final/en_US/en_US.twitter.txt', 100000)
})

system.time({
  process.txts(txts)
})

txts.dt <- data.table(words=txts)

## 20 s
system.time({
  removePunctuation(txts)
})

system.time({
  txts2.dt <- txts.dt[,.(removePunct(words)),]
})

txts2.dt
class(txts2.dt)
system.time({
  txts3.dt <- txts.dt[,removePunctuation(words),]
})


txts.dt
txts2.dt

txts <- txts.process(txts)

## aprx 2
tokens <- txts.tokenization(txts)
system.time({
  tokens.dt <- words.freq(tokens)
})

```

```{r}
ggplot(data=tokens.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip() 
```

```{r}
##wordcloud(tokens, scale=c(5,0.5),
##          max.words=300, random.order=FALSE,
##          rot.per=0.35, use.r.layout=FALSE,
##          colors=brewer.pal(8, "Dark2"))
```

## Frequencies of 2-grams

```{r}


grams2 <- NGramTokenizer(txts, Weka_control(min = 2, max = 2))
grams2.tb <- sort(table(grams2), decreasing = TRUE)
grams2.dt <- words.freq(grams2)

ggplot(grams2.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()

```

## Frequencies of 3-grams


```{r}
grams3 <- NGramTokenizer(txts, Weka_control(min = 3, max = 3))
grams3.dt <- words.freq(grams3)
ggplot(grams3.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```

## Frequencies of 4-grams


```{r}
grams4 <- NGramTokenizer(txts, Weka_control(min = 4, max = 4))
grams4.dt <- words.freq(grams4)
ggplot(grams4.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```




```{r}

## d <- 0.5

## word <- "love"
## word.re <- paste0("^",word," ")
## grams2.rel <- grams2[grep(word.re, grams2)]
##words.next <- gsub(word.re, "", grams2.rel)
##words.next.tb <- sort(table(words.next), decreasing = TRUE)
##words.next.tb[1:20]


##system.time({
##  word <- "love"
##  word.re <- paste0("^",word," ")
##  grams2.rel <- grams2[grep(word.re, grams2)]
##  words.next <- gsub(word.re, "", grams2.rel)
##  words.next.tb <- sort(table(words.next), decreasing = TRUE)
##  words.next.tb[1:20]
##})

```
