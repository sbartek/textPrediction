# Text Prediction


```{r libraries}
source('textPred.R')

require('ggplot2')
require('gridExtra')
require('wordcloud')
```

## Data acquisition and cleaning


### Obtaining the data

First we download and clean data with the following script.

```{r downloadData}
if (!file.exists('data')) dir.create('data')
zip.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zip.fn <- "data/Coursera-SwiftKey.zip"
if (!file.exists(zip.fn)) download.file(zip.url, destfile=zip.fn, method="curl")
if (!file.exists('data/final')) unzip(zip.fn, exdir='data')
```

### Cleaning

Now it is time for cleaning. We will read the file that contains
tweets in English. Then we transform the vector into data.table, since
the operations we will do later results to be faster. We will also
cache the data.table file.

```{r readData}
read.datafile <- function(fn, n=-1L) {
  txts <- readLines(fn, n)
  data.table(words=txts)
}
```

Next, we lower case of all letter, and then we deal with is
punctuation. What we are going to treat symbols `. , ? ... ; ! : () "`
as the one that divide the message (another future strategy is to only
remove them). We also include here a lonely `-`. Then we remove extra
empty spaces and finally we tokenize them. Here we use function
`basicDT` implemented in separated file.


```{r tokenization}
getTokens <- function(vn, fn, n=-1L) {
  if (dbExists(dbCache, vn)) {
    v <- dbFetch(dbCache, vn)
  } else {
    v <- read.datafile(fn, n) %>>%
      basicDT()
    dbInsert(dbCache, vn, v)
  }
  v
}
```

```{r tus}
tus.tokens <-
  getTokens("tus.tokens", 
            "data/final/en_US/en_US.twitter.txt", n=-1L)
bus.tokens <-
  getTokens("bus.tokens", 
            "data/final/en_US/en_US.blogs.txt", n=-1L)
nus.tokens <-
  getTokens("nus.tokens", 
            "data/final/en_US/en_US.news.txt", n=-1L)
```

## Exploratory analysis

### Frequencies of tokens

```{r freq}
tokens.freq <- function(dt) {
  dt[,.(counts=.N),by=tokens1][order(-counts)]
}
```

```{r}
tus.freq <- tokens.freq(tus.tokens)
bus.freq <- tokens.freq(bus.tokens)
nus.freq <- tokens.freq(nus.tokens)
```

```{r}
plotFreq <- function(dt) {
  p <- ggplot(dt) +
    geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
    xlab("Words")+ylab('Frequencies') +
    coord_flip()
}
```

```{r, fig.cap="The most frequnts words in (from left) tweets, blogs, news"}
pt <- plotFreq(tus.freq[1:40, .(words=tokens1, counts)])
pb <- plotFreq(bus.freq[1:40, .(words=tokens1, counts)])
pn <- plotFreq(nus.freq[1:40, .(words=tokens1, counts)])
grid.arrange(pt, pb, pn, ncol=3)
```

```{r worldcloud, fig.cap="Most popular words in tweets"}

wordcloud(tus.freq[1:500, tokens1], tus.freq[1:500, counts],
          scale=c(5,0.5),
          random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

### Ngrams

```{r}

tus.ngrams <- ngramDTs(tus.tokens, 3)

## system.time({
## if (dbExists(dbCache, "tus.ngrams")) {
##   print('from db...')
##   tus.ngrams <- dbFetch(dbCache, "tus.ngrams")
## } else {
##   tus.ngrams <- ngramDTs(tus.tokens, 5)
##   dbInsert(dbCache, "tus.ngrams", tus.ngrams)
## }
## })

#dbInsert(dbCache, "tus.ngrams5", tus.ngrams[[5]])

```



```{r}





system.time({
  tus.test <- read.datafile('data/final/en_US/en_US.twitter.txt', 100000)
  tus2 <- tus.test[,.(words=txts.clean(words))][words!=""][,id:=.I]
  tus3b <- tus2[,.(tokens1=txts.tokens(words)), by=id]

})

N <- dim(tus3b)[1]




tus4b <- cbind(tus3b[1:N-1] , tus3b[2:N])
setnames(tus4b, c(colnames(tus3b), "id2", "tokens2"))
class(tus4b)
tus4b <- tus4b[get("id2")==id]
tus4b <- tus4b[,!"id2", with=FALSE]


setnames(tus4b, c("id1", "tokens1", "id2", "tokens2"))
tus5b <- tus4b


txts.ngrams <- function(txts, n=2) {
  unlist(lapply(txts, function(txt) get.ngrams(ngram(txt, n))))
}


system.time({
  ## tus30 <- tus20[,counts:=txts.count(words)]
  
  tus5 <- tus3[1:10000][counts>1,.(words=txts.ngrams(words, 2))]  
})



system.time({
  tus3 <- tus2[,.(words=txts.ngrams(words, 1))]
})



BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(x, 2),
                paste, collapse = " "), use.names =FALSE)
}
```

```{r processingFunctions}




removePunct <- function(txts) {
  gsub("[[:punct:]]", "", txts)
}

process.txts <- function(txts) {
  txts %>>%
    removePunctuation %>>%
    tolower %>>%
    removeNumbers %>>%
    removeWords(stopwords('english')) %>>%
    stripWhitespace %>>%
    trim
}

txts.tokenization <- function(txts) {
  txts  %>>%
    MC_tokenizer %>>%
    (.[.!=""])
}

words.freq <- function(words) {
  words.dt <- data.table(words=words)
  words.dt[,.(counts=.N),by=words][order(-counts)]
}

```

## Frequencies of tokens

```{r load_files}

## aprox 11 s

system.time({
  txts <- readLines('data/final/en_US/en_US.twitter.txt', 100000)
})

system.time({
  process.txts(txts)
})

txts.dt <- data.table(words=txts)

## 20 s
system.time({
  removePunctuation(txts)
})

system.time({
  txts2.dt <- txts.dt[,.(removePunct(words)),]
})

txts2.dt
class(txts2.dt)
system.time({
  txts3.dt <- txts.dt[,removePunctuation(words),]
})


txts.dt
txts2.dt

txts <- txts.process(txts)

## aprx 2
tokens <- txts.tokenization(txts)
system.time({
  tokens.dt <- words.freq(tokens)
})

```

```{r}
ggplot(data=tokens.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip() 
```

```{r}
##wordcloud(tokens, scale=c(5,0.5),
##          max.words=300, random.order=FALSE,
##          rot.per=0.35, use.r.layout=FALSE,
##          colors=brewer.pal(8, "Dark2"))
```

## Frequencies of 2-grams

```{r}


grams2 <- NGramTokenizer(txts, Weka_control(min = 2, max = 2))
grams2.tb <- sort(table(grams2), decreasing = TRUE)
grams2.dt <- words.freq(grams2)

ggplot(grams2.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()

```

## Frequencies of 3-grams


```{r}
grams3 <- NGramTokenizer(txts, Weka_control(min = 3, max = 3))
grams3.dt <- words.freq(grams3)
ggplot(grams3.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```

## Frequencies of 4-grams


```{r}
grams4 <- NGramTokenizer(txts, Weka_control(min = 4, max = 4))
grams4.dt <- words.freq(grams4)
ggplot(grams4.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```




```{r}

## d <- 0.5

## word <- "love"
## word.re <- paste0("^",word," ")
## grams2.rel <- grams2[grep(word.re, grams2)]
##words.next <- gsub(word.re, "", grams2.rel)
##words.next.tb <- sort(table(words.next), decreasing = TRUE)
##words.next.tb[1:20]


##system.time({
##  word <- "love"
##  word.re <- paste0("^",word," ")
##  grams2.rel <- grams2[grep(word.re, grams2)]
##  words.next <- gsub(word.re, "", grams2.rel)
##  words.next.tb <- sort(table(words.next), decreasing = TRUE)
##  words.next.tb[1:20]
##})

```
