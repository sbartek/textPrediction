## Introduction

```{r}
require("tm")
require("pipeR")
require('wordcloud')
require("data.table")
require('ggplot2')
```

### Cache

```{r}
require(filehash)
cache.fn <- "cache/dbCache"
if (!file.exists('cache')) dir.create('cache')
if (!file.exists(cache.fn)) dbCreate(cache.fn)
dbCache <- dbInit(cache.fn)
```

## Data acquisition and cleaning


### Obtaining the data

```{r}

if (!file.exists('data')) dir.create('data')
zip.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zip.fn <- "data/Coursera-SwiftKey.zip"
if (!file.exists(zip.fn)) download.file(zip.url, destfile=zip.fn, method="curl")
if (!file.exists('data/final')) unzip(zip.fn, exdir='data')

```

```{r}
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

load.txts <- function(fn) {
  readLines(fn) %>>%
    removePunctuation %>>%
    tolower %>>%
    removeNumbers %>>%
    removeWords(stopwords('english')) %>>%
    stripWhitespace %>>%
    trim
}

txts.tokenization <- function(txts) {
  txts  %>>%
    MC_tokenizer %>>%
    (.[.!=""])
}

words.freq <- function(words) {
  words.dt <- data.table(words=words)
  words.dt[,.(counts=.N),by=words][order(-counts)]
}

```

## Frequencies of tokens

```{r}
txts <- load.txts('data/final/en_US/en_US.twitter.txt')
tokens <- txts.tokenization(txts)
tokens.dt <- words.freq(tokens)
```

```{r}
ggplot(data=tokens.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip() 
```

```{r}
##wordcloud(tokens, scale=c(5,0.5),
##          max.words=300, random.order=FALSE,
##          rot.per=0.35, use.r.layout=FALSE,
##          colors=brewer.pal(8, "Dark2"))
```

## Frequencies of 2-grams

```{r}

require('RWeka')

grams2 <- NGramTokenizer(txts, Weka_control(min = 2, max = 2))
grams2.tb <- sort(table(grams2), decreasing = TRUE)
grams2.dt <- words.freq(grams2)

ggplot(grams2.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()

```

## Frequencies of 3-grams


```{r}
grams3 <- NGramTokenizer(txts, Weka_control(min = 3, max = 3))
grams3.dt <- words.freq(grams3)
ggplot(grams3.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```

## Frequencies of 4-grams


```{r}
grams4 <- NGramTokenizer(txts, Weka_control(min = 4, max = 4))
grams4.dt <- words.freq(grams4)
ggplot(grams4.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```




```{r}

## d <- 0.5

## word <- "love"
## word.re <- paste0("^",word," ")
## grams2.rel <- grams2[grep(word.re, grams2)]
##words.next <- gsub(word.re, "", grams2.rel)
##words.next.tb <- sort(table(words.next), decreasing = TRUE)
##words.next.tb[1:20]


##system.time({
##  word <- "love"
##  word.re <- paste0("^",word," ")
##  grams2.rel <- grams2[grep(word.re, grams2)]
##  words.next <- gsub(word.re, "", grams2.rel)
##  words.next.tb <- sort(table(words.next), decreasing = TRUE)
##  words.next.tb[1:20]
##})

```
