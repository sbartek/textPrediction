# Text Prediction


## Introduction

```{r libraries}
require("tm")
require("pipeR")
require('wordcloud')
require("data.table")
require('ggplot2')
require('filehash')
require('ngram')
require('RWeka')
```

### Cache

```{r filecache}
cache.fn <- "cache/dbCache"
if (!file.exists('cache')) dir.create('cache')
if (!file.exists(cache.fn)) dbCreate(cache.fn)
dbCache <- dbInit(cache.fn)
```

## Data acquisition and cleaning


### Obtaining the data

First we download and clean data with the following script.

```{r downloadData}
if (!file.exists('data')) dir.create('data')
zip.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zip.fn <- "data/Coursera-SwiftKey.zip"
if (!file.exists(zip.fn)) download.file(zip.url, destfile=zip.fn, method="curl")
if (!file.exists('data/final')) unzip(zip.fn, exdir='data')
```

### Cleaning

Now it is time for cleaning. We will read
10000 tweets in English in order to see what we have, since reading
entire file takes about 3 minutes on my computer. Then we transform
the vector into data.table, since the operations we will do later
results to be faster.

```{r}
read.datafile <- function(fn, n=-1L) {
  txts <- readLines(fn, n)
  data.table(words=txts)
}

tus <- read.datafile('data/final/en_US/en_US.twitter.txt')
tus.test <- read.datafile('data/final/en_US/en_US.twitter.txt', 10000)
```

Next, we lower case of all letter, and then we deal with is
punctuation. What we are going to treat symbols `. , ? ... ; ! : () "`
as the one that divide the message (another future strategy is to only
remove them). We also include here a lonely `-`. Then we remove
extra empty spaces and finally we tokenize them.

```{r}
source('textPred.R')

system.time({
  tus.test.tokens <- basicDT(tus.test)
})

if (dbExists(dbCache, "tus.tokens")) {
  tus.tokens <- dbFetch(dbCache, "tus.tokens")
} else {
  tus.tokens <- basicDT(tus)
  dbInsert(dbCache, "tus.tokens", tus.tokens)
}

system.time({
  tus1.freq <- tus.tokens[,.(N=.N),by=tokens1][order(-N)]
})

ggplot(tus1.freq[1:40]) +
  geom_bar(aes(x=reorder(tokens1, N), y=N), stat="identity") +
  xlab("tokens")+ylab('occurances')
  coord_flip()
```


```{r}





system.time({
  tus.test <- read.datafile('data/final/en_US/en_US.twitter.txt', 100000)
  tus2 <- tus.test[,.(words=txts.clean(words))][words!=""][,id:=.I]
  tus3b <- tus2[,.(tokens1=txts.tokens(words)), by=id]

})

N <- dim(tus3b)[1]
newtus3b <- tus3b



tus4b <- cbind(tus3b[1:N-1] , tus3b[2:N])
setnames(tus4b, c(colnames(tus3b), "id2", "tokens2"))
class(tus4b)
tus4b <- tus4b[get("id2")==id]
tus4b <- tus4b[,!"id2", with=FALSE]


setnames(tus4b, c("id1", "tokens1", "id2", "tokens2"))
tus5b <- tus4b


txts.ngrams <- function(txts, n=2) {
  unlist(lapply(txts, function(txt) get.ngrams(ngram(txt, n))))
}


system.time({
  ## tus30 <- tus20[,counts:=txts.count(words)]
  
  tus5 <- tus3[1:10000][counts>1,.(words=txts.ngrams(words, 2))]  
})



system.time({
  tus3 <- tus2[,.(words=txts.ngrams(words, 1))]
})



BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(x, 2),
                paste, collapse = " "), use.names =FALSE)
}
```

```{r processingFunctions}




removePunct <- function(txts) {
  gsub("[[:punct:]]", "", txts)
}

process.txts <- function(txts) {
  txts %>>%
    removePunctuation %>>%
    tolower %>>%
    removeNumbers %>>%
    removeWords(stopwords('english')) %>>%
    stripWhitespace %>>%
    trim
}

txts.tokenization <- function(txts) {
  txts  %>>%
    MC_tokenizer %>>%
    (.[.!=""])
}

words.freq <- function(words) {
  words.dt <- data.table(words=words)
  words.dt[,.(counts=.N),by=words][order(-counts)]
}

```

## Frequencies of tokens

```{r load_files}

## aprox 11 s

system.time({
  txts <- readLines('data/final/en_US/en_US.twitter.txt', 100000)
})

system.time({
  process.txts(txts)
})

txts.dt <- data.table(words=txts)

## 20 s
system.time({
  removePunctuation(txts)
})

system.time({
  txts2.dt <- txts.dt[,.(removePunct(words)),]
})

txts2.dt
class(txts2.dt)
system.time({
  txts3.dt <- txts.dt[,removePunctuation(words),]
})


txts.dt
txts2.dt

txts <- txts.process(txts)

## aprx 2
tokens <- txts.tokenization(txts)
system.time({
  tokens.dt <- words.freq(tokens)
})

```

```{r}
ggplot(data=tokens.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip() 
```

```{r}
##wordcloud(tokens, scale=c(5,0.5),
##          max.words=300, random.order=FALSE,
##          rot.per=0.35, use.r.layout=FALSE,
##          colors=brewer.pal(8, "Dark2"))
```

## Frequencies of 2-grams

```{r}


grams2 <- NGramTokenizer(txts, Weka_control(min = 2, max = 2))
grams2.tb <- sort(table(grams2), decreasing = TRUE)
grams2.dt <- words.freq(grams2)

ggplot(grams2.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()

```

## Frequencies of 3-grams


```{r}
grams3 <- NGramTokenizer(txts, Weka_control(min = 3, max = 3))
grams3.dt <- words.freq(grams3)
ggplot(grams3.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```

## Frequencies of 4-grams


```{r}
grams4 <- NGramTokenizer(txts, Weka_control(min = 4, max = 4))
grams4.dt <- words.freq(grams4)
ggplot(grams4.dt[1:20]) +
  geom_bar(aes(x=reorder(words, counts), y=counts), stat="identity") +
  coord_flip()
```




```{r}

## d <- 0.5

## word <- "love"
## word.re <- paste0("^",word," ")
## grams2.rel <- grams2[grep(word.re, grams2)]
##words.next <- gsub(word.re, "", grams2.rel)
##words.next.tb <- sort(table(words.next), decreasing = TRUE)
##words.next.tb[1:20]


##system.time({
##  word <- "love"
##  word.re <- paste0("^",word," ")
##  grams2.rel <- grams2[grep(word.re, grams2)]
##  words.next <- gsub(word.re, "", grams2.rel)
##  words.next.tb <- sort(table(words.next), decreasing = TRUE)
##  words.next.tb[1:20]
##})

```
